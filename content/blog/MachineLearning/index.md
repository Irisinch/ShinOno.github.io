---
title: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯
summary: æ©Ÿæ¢°å­¦ç¿’ã®è§£èª¬ã¨å®Ÿè£…ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ä»˜ãã€‚
date: 2024-12-09
authors:
  - admin
tags:
  - MachineLearning
---


## æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦
æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¾ã›ã€ãƒ‡ãƒ¼ã‚¿å†…ã«æ½œã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®ãƒ«ãƒ¼ãƒ«ã‚’ç²å¾—ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ãƒ‡ãƒ¼ã‚¿è§£ææŠ€è¡“ã§ã‚ã‚‹ã€‚  
æ©Ÿæ¢°å­¦ç¿’ã®ä¸»è¦ãªç›®çš„ã¯ã€Œäºˆæ¸¬ã€ã«ã‚ã‚‹ã€‚æ©Ÿæ¢°å­¦ç¿’ã®æ–‡è„ˆã§ã®äºˆæ¸¬ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å¤‰æ•°åŒå£«ã®é–¢ä¿‚ã‚’æŠ½å‡ºã—ã€ãã®é–¢ä¿‚ã‚’æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å½“ã¦ã¯ã‚ã‚‹ã“ã¨ã§ã€ç‰¹å®šã®å¤‰æ•°ã®å€¤ã‚’æ¨å®šã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚ã¾ãŸã€ã“ã®äºˆæ¸¬ã¯ä¸€èˆ¬çš„ãªæ³•å‰‡æ€§ã‚’çŸ¥ã‚‹ã“ã¨ã¨ã¯ç•°ãªã‚‹ã€‚ç‰¹ã«ã€äºˆæ¸¬ç²¾åº¦ã®é«˜ã„æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®å¤šãã¯ã€äººé–“ãŒè§£é‡ˆã§ãã‚‹ã‚ˆã†ãªçŸ¥è­˜ã‚’å¾—ã‚‰ã‚Œãªã„ã€Œãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã€ã¨ãªã£ã¦ã„ã‚‹ã€‚ã“ã®ç‚¹ã§ã€çµ±è¨ˆè§£æã¨ã®å¤§ããªé•ã„ãŒã‚ã‚‹ã€‚<br>
<br>
â€» æ©Ÿæ¢°å­¦ç¿’ã¨æ··åŒã•ã‚Œã‚‹æ¦‚å¿µã«AIã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ·±å±¤å­¦ç¿’ï¼‰ãŒã‚ã‚‹ã€‚AIã¨ã¯äººé¡ã®çŸ¥èƒ½ã‚’å†ç¾ã™ã‚‹è©¦ã¿å…¨èˆ¬ã®ã“ã¨ã§ã‚ã‚Šã€æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†…åŒ…ã™ã‚‹ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®ä¸€ç¨®ã®æ‰‹æ³•ï¼ˆå¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ç•°ãªã‚‹ã‚‚ã®ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚  
<br>
æ©Ÿæ¢°å­¦ç¿’ã«ãŠã„ã¦ã€Œå­¦ç¿’ã€ã®æ ¸ã¨ãªã‚‹ã®ã¯ã€ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®èª¿æ•´ã§ã‚ã‚‹ã€‚ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã¯ã€ä»®èª¬ã«åŸºã¥ãåˆ¶ç´„ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã®ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦è‡ªç”±ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã„ãéç¨‹ã«è¿‘ã„ã€‚ãã®ãŸã‚ã€ã©ã“ã¾ã§ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã‚‹ã‹ãŒé‡è¦ã¨ãªã‚‹ã€‚ï¼ˆéå­¦ç¿’ãƒ»å­¦ç¿’ä¸è¶³ï¼‰  
æ©Ÿæ¢°å­¦ç¿’ã§ä½¿ã‚ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€åˆ†æè€…ãŒãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚é€šå¸¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ã®å½¢ã‚’åˆ¶å¾¡ã™ã‚‹ä¸€æ–¹ã§ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºã‚ã‚‹ãŸã‚ã®æ¡ä»¶ã‚’èª¿æ•´ã™ã‚‹ã€‚ï¼ˆ4. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ï¼‰  
<br>

## æ©Ÿæ¢°å­¦ç¿’ã®ã‚¹ãƒ†ãƒƒãƒ—
![screen reader text](MLflow.jpg)<br>
### 1. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆsplitï¼‰
ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’è©•ä¾¡ç”¨ã€æ®‹ã‚Šã‚’å­¦ç¿’ï¼ˆãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼‰ç”¨ã«åˆ†ã‘ã‚‹ã€‚ã“ã‚Œã¯ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚é©åˆ‡ãªäºˆæ¸¬çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ï¼ˆæ±åŒ–æ€§èƒ½ï¼‰ã‚’é‡è¦–ã™ã‚‹ãŸã‚è¡Œã‚ã‚Œã‚‹ã€‚
ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã¯ä¸»ã«2å›è¡Œã‚ã‚Œã€æœ€åˆã®åˆ†å‰²ã§ã¯æœ€çµ‚çš„ãªç²¾åº¦ã‚’è©¦ã™ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™ã€‚æ¬¡ã®åˆ†å‰²ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«èª¿æ•´ã™ã‚‹ãŸã‚ã«å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨èª¿æ•´å…·åˆã‚’ç¢ºã‹ã‚ã‚‹ï¼ˆæ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ï¼‰ã«åˆ†å‰²ã™ã‚‹ã€‚  
ä»¥ä¸‹ã€åˆ†å‰²ã®æ‰‹æ³•ã§ã‚ã‚‹ã€‚
* ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆæ³•  
ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¤šã„æ™‚ã«è¡Œã‚ã‚Œã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ä»»æ„ã®å‰²åˆã§åˆ†å‰²ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹ã€‚  
* ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°  
ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã€æŠ½å‡ºã•ã‚Œãªã‹ã£ãŸã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã¨ãƒ†ã‚¹ãƒˆã‚’è¤‡æ•°å›è¡Œã„ã€å…¨è©¦è¡Œã®å¹³å‡ã‚’æœ€çµ‚çš„ãªè©•ä¾¡çµæœã¨ã—ã¦æ¡ç”¨ã™ã‚‹ã€‚  
* ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäº¤å·®æ¤œè¨¼ï¼‰æ³•  
ä½•åº¦ã‹ç•°ãªã‚‹åˆ†å‰²ã‚’è¡Œã„ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ç•°ãªã‚‹çµ„ã¿åˆã‚ã›ã‚’å¾—ã‚‹æ–¹æ³•ã€‚å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã•ã‚Œã‚‹ç‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ç•°ãªã‚‹ã€‚  
åˆ†å‰²æ•°ã‚’kã§ç¤ºã—ã€kå±¤åˆ†å‰²äº¤å·®æ¤œè¨¼ï¼ˆk-folds cross validationï¼‰ã¨å‘¼ã¶ã€‚  
<br>
ä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã¯ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆæ³•ã§åˆ†å‰²ã—ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§è¡Œã£ã¦ã„ã‚‹ã€‚

### 2. å­¦ç¿’
æ©Ÿæ¢°å­¦ç¿’ã®åˆ©ç”¨æ–¹æ³•ã¯ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨æ•™å¸«ãªã—å­¦ç¿’ã«å¤§åˆ¥ã•ã‚Œã‚‹ã€‚  
æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯**å›å¸°**ã¨**åˆ†é¡**ã«åˆ†ã‹ã‚Œã‚‹ã€‚  
æ•™å¸«ãªã—å­¦ç¿’ã¯**ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°**ã¨**æ¬¡å…ƒå‰Šæ¸›**ã«åˆ†ã‹ã‚Œã‚‹ã€‚
å­¦ç¿’ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠãƒ»èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã®é¸æŠãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®šã‚’è¡Œã†å¿…è¦ãŒã‚ã‚‹ã€‚  
**ä»¥ä¸‹ã¯åˆ†é¡å•é¡Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿ã®ç´¹ä»‹ã§ã‚ã‚‹**

#### æ­£å‰‡åŒ–ç·šå½¢å›å¸°
* ElasticNet (Ridge, Lasso)  
ã•ã¾ã–ã¾ãªç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãªã‚ã‚‰ã‹ã«ã—ãŸã‚Šã€å½±éŸ¿ã®å°ã•ã„å¤‰æ•°ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã‚‹ã€‚  
[glmnet](https://glmnet.stanford.edu/articles/glmnet.html)   

* group Elastic Net (LASSO)  
ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«å¤‰æ•°ã‚’æ¸›ã‚‰ã›ã‚‹ 
[grpnet](https://cran.r-project.org/web/packages/grpnet/index.html)

#### å¢ƒç•Œåˆ†é¡
* SVMï¼ˆSupport Vector Machineï¼‰  
ç©ºé–“ã«å¢ƒç•Œã‚’ä½œã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡ã™ã‚‹ã€‚  
éç·šå½¢ãªåˆ†é¡ã‚‚å¯èƒ½ã§ã‚ã‚‹ã€‚  
[ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://colab.research.google.com/drive/17iVNlxBAO6eKqJaew8-H_nviWbNDsjpJ?usp=sharing)

#### æ±ºå®šæœ¨ãƒ™ãƒ¼ã‚¹
* æ±ºå®šæœ¨  
ä¸€ç•ªåŸºç¤çš„ãªæ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ã€‚æœ¨ã«ä¼¼ãŸãƒ•ãƒ­ãƒ¼ã‚’ä½œã‚‹ã“ã¨ã§åˆ†é¡ã™ã‚‹ã€‚  
[ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://colab.research.google.com/drive/165NfKJRO89UMDwl5yMOAeD6wuZcFqA16?usp=sharing)
* ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ  
ãŸãã•ã‚“ã®æœ¨ã‚’ä½œã‚Šã€ãã‚Œã‚‰ã®å¤šæ•°æ±ºã‚’è¡Œã†ã“ã¨ã§åˆ†é¡ã™ã‚‹ã€‚  
[ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://colab.research.google.com/drive/1cNyNKNs5jaCD0RnGgvYfBfhqn8jy56Xh?usp=sharing)
* LightGBM  
æ®µéšã”ã¨ã«æœ¨ã‚’æ·±ãã—ã¦è¡Œãã€‚å‰ã®æ®µéšã§ç²¾åº¦ã®è‰¯ããªã„ãƒãƒ¼ãƒ‰ã‹ã‚‰æã‚’ç”Ÿã‚„ã™ã“ã¨ã§ç²¾åº¦ã‚’ä¸Šã’ã¦è¡Œãã€‚
* XGBoost  
æ®µéšã”ã¨ã«æœ¨ã‚’æ·±ãã—ã¦è¡Œãã€‚å‰ã¾ã§ã®æœ¨ã§ã†ã¾ãåˆ†é¡ã§ããªã‹ã£ãŸç®‡æ‰€ã‚’åŸ‹ã‚ã‚‹ã‚ˆã†ã«æ¬¡ã®æœ¨ã‚’ä½œã£ã¦è¡Œãã€‚  
[ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://colab.research.google.com/drive/1NdQntsLuqOBJVFjngdVUWqZn1-MUszuz?usp=sharing)

#### ğŸš§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ğŸš§
* åŸºæœ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯  
[ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://colab.research.google.com/drive/1ouVsJgs3CIpDOj_0mZ_rToG5E9ypd0kV?usp=sharing)
<<Note(ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã¯GPUã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚  
æ™®æ®µã‚ˆã‚Šåˆ©ç”¨ã§ãã‚‹æ™‚é–“ãƒ»è¨ˆç®—é‡ã«åˆ¶é™ãŒã‚ã‚‹ã®ã§æ³¨æ„ã—ã¦ä½¿ã£ã¦ãã ã•ã„ã€‚)>>
* ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰  
* å†èµ·å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰  



### 3. è©•ä¾¡
å­¦ç¿’ã«ç”¨ã„ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’ç¢ºèªã™ã‚‹ã€‚  

#### åˆ†é¡å•é¡Œ  
åˆ†é¡å•é¡Œã§ã¯ã€æ··åŒè¡Œåˆ—ã‚’ä½œæˆã—ã¦ã‹ã‚‰æ§˜ã€…ãªæŒ‡æ¨™ã‚’ç¢ºèªã™ã‚‹ã€‚  
|                | Predicted Positive | Predicted Negative |
|----------------|--------------------|--------------------|
| **Actual Positive** | TP (True Positive)                 | FN (False Negative)                 |
| **Actual Negative** | FP (False Positive)                  | TN (True Negative)                 |

* æ­£è§£ç‡  
å…¨ã¦ã®äºˆæ¸¬ã®ã†ã¡ã€æ­£è§£ã—ãŸäºˆæ¸¬ã®å‰²åˆ  
TP + TN / TP + TN + FP + FN
* é©åˆç‡  
é™½æ€§ã¨äºˆæ¸¬ã—ãŸã‚‚ã®ã®ã†ã¡ã€å®Ÿéš›ã«é™½æ€§ã§ã‚ã‚‹ã‚‚ã®ã®å‰²åˆ
TP / TP + FP
* å†ç¾ç‡  
å®Ÿéš›ã«é™½æ€§ã§ã‚ã‚‹ã‚‚ã®ã®ã†ã¡ã€æ­£ã—ãé™½æ€§ã¨äºˆæ¸¬ã§ããŸã‚‚ã®ã®å‰²åˆ
TP / TP + FN
* Få€¤  
é©åˆç‡ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡
2 Ã— é©åˆç‡ Ã— å†ç¾ç‡ / é©åˆç‡ + å†ç¾ç‡
* AUC-ROC  
ã‚µãƒ³ãƒ—ãƒ«ã‚’æ­£ã—ãä¸¦ã³æ›¿ãˆã‚‹ã“ã¨ãŒã§ããŸã‹ã‚’ç¤ºã™  
[å‚è€ƒ](https://arize.com/blog/what-is-auc/)
* AUC-PR  
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®éš›ã«ç”¨ã„ã‚‰ã‚Œã‚‹ã€‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒä¸Šä½ã®ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬ã®æ­£ç¢ºã•ã‚’ã‚ˆã‚Šé‡è¦–ã™ã‚‹ã€‚

#### å›å¸°å•é¡Œ
* å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰  
* å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰  
* å¹³å‡çµ¶å¯¾èª¤å·®ç‡ï¼ˆMAPEï¼‰  

### 4. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
é«˜ã„äºˆæ¸¬ç²¾åº¦ã‚’å¾—ã‚‹ãŸã‚ã«å­¦ç¿’ã®éš›ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ã‚’å¤‰ãˆã¦ã€å­¦ç¿’ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹è©•ä¾¡ã‚’ç¹°ã‚Šè¿”ã™ã€‚
* ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ  
ä¸ãˆã‚‰ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œã®å€¤ã®å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚’è¡Œã†æ‰‹æ³•ã§ã‚ã‚‹ã€‚  

```
param_grid = {
    "X": ["A",  "B"], 
    "Y": [i / 10 for i in range(10, 20, 2)],  # out -> [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]
}
```

ä¸Šè¨˜ã®å ´åˆã€2 * 10 ã®20ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ¢ãƒ‡ãƒ«ãŒæ§‹ç¯‰ã•ã‚Œã€æœ€ã‚‚ç²¾åº¦ã®é«˜ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé¸ã°ã‚Œã‚‹ã€‚

* ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ  
ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§ã¯ç¶²ç¾…çš„ã«çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç·»å¯†ã«èª¿æ•´ã—ã‚ˆã†ã¨æ€ã†ã¨è†¨å¤§ãªæ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã†ã€‚  
ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¯ã€æŒ‡å®šã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ã«å€¤ã‚’é¸æŠã—ã€ãã®çµ„ã¿åˆã‚ã›ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚  
ãƒ©ãƒ³ãƒ€ãƒ ãªçµ„ã¿åˆã‚ã›ã‚’é¸æŠã™ã‚‹ãŸã‚ã€å±€æ‰€æœ€é©ã«é™¥ã‚‰ãšã€çŸ­æ™‚é–“ã§è‰¯å¥½ãªçµ„ã¿åˆã‚ã›ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

* ãƒ™ã‚¤ã‚ºæœ€é©åŒ–  
ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ æ€§ã«é ¼ã‚‰ãšã€æœ€å°é™ã®è©¦è¡Œã§æœ€é©ãªè§£ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
æœªçŸ¥ã®ç›®çš„é–¢æ•°ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ãˆã°ã‚¬ã‚¦ã‚¹éç¨‹ï¼‰ã‚’ä½¿ç”¨ã—ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸Šã§äº‹å¾Œåˆ†å¸ƒã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€æ¬¡ã®è©¦è¡Œç‚¹ã‚’é¸æŠã™ã‚‹ã€‚  
**ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã«ã¯optunaã§ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚**

### ğŸš§ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ğŸš§


## About Machine Learning
Machine learning is a data analysis technique that enables the acquisition of rules for making decisions about unknown data by reading large amounts of data and learning patterns latent in the data.  
The primary goal of machine learning is "Prediction". Prediction in the context of machine learning is to estimate the value of a particular variable by extracting the relationship between variables in the data and applying that relationship to new data. This "prediction" is also different from knowing the general rule of thumb. In particular, many machine learning methods with high prediction accuracy are â€œblack boxesâ€ that do not yield knowledge that can be interpreted by humans. In this respect, there is a major difference between statistical analysis. 
<br><br>
â€» AI and deep learning are concepts that are confused with machine learning. AI refers to any attempt to reproduce human intelligence, and includes machine learning and deep learning. Deep learning is a type of machine learning technique (multilayer neural network), and these refer to different things.
<br><br>
The core of "learning" in machine learning is fitting adjustment. Fitting is more like the process of freely creating a model to fit the data, rather than creating a model with constraints based on a hypothesis. Therefore, the extent to which it fits the data is important. (Overfitting, Underfitting)
<br><br>
Algorithms used in machine learning require analysts to specify hyperparameters. While regular parameters control the shape of the model, hyperparameters adjust the conditions for determining the parameters. (Continue to 4. Tuning)
<br><br>

## Steps of Machine Learning
![screen reader text](MLflow.jpg)<br>
### 1. Data Split
A portion of the data is divided for evaluation and the remainder for training (fitting). This is done to emphasize the importance of obtaining appropriate prediction results even for new data (generalization performance).
The data is split twice, the first split being the test data for the final accuracy. In the next split, the data is split into training data and validation data in order to adjust the hyperparameters to the best fit for the data.  
The following is the method of division.  
* Hold-out method  
This is done when the number of samples is large. Data is splot into test data and data for verification at an arbitrary ratio.  
* Random sampling  
Randomly extract data to construct a training data set, and use the unextracted samples for testing. Construct and test the dataset multiple times, and adopt the average of all trials as the final evaluation result.  
* Cross-validation method  
A method of obtaining different combinations of training and validation data by making several different splits. It differs from random sampling in that all data are used for testing.  
The number of splits is denoted by k and is called k-folds cross validation (k-folds cross validation).  
<br>
In the sample code below, test data is split using the holdout method and tuning is done by cross validation.

### 2. Machine Learning Methods (algorithms)
The algorithms of machine learning can be broadly divided into supervised and unsupervised learning.  
Supervised learning is divided into **regression** and **classification**.  
Unsupervised learning is divided into **clustering** and **dimension reduction**.
In this step, it is necessary to select the algorithm, the explanatory and objective variables, and the hyperparameters.  
**The following is an introduction to the classification problem algorithms only**.

#### Regularized linear regression
* ElasticNet (Ridge, Lasso)  
Combined with various linear regression models, the model can be smoothed or reduced to variables with small effects.  
[glmnet](https://glmnet.stanford.edu/articles/glmnet.html)   

* group Elastic Net (LASSO)  
Can reduce variables per group  
[grpnet](https://cran.r-project.org/web/packages/grpnet/index.html)

#### Boundary classification
* SVMï¼ˆSupport Vector Machineï¼‰  
Classifies data by creating boundaries in space.  
Non-linear classification is also possible.  
[Sample code](https://colab.research.google.com/drive/17iVNlxBAO6eKqJaew8-H_nviWbNDsjpJ?usp=sharing)

#### Decision tree based models
* Decision tree
The most basic decision tree model. Classification is done by creating tree-like flows.  
[Sample Code](https://colab.research.google.com/drive/165NfKJRO89UMDwl5yMOAeD6wuZcFqA16?usp=sharing)
* Random Forest  
Classify by creating many decision trees and making majority decisions on them.  
[Sample Code](https://colab.research.google.com/drive/1cNyNKNs5jaCD0RnGgvYfBfhqn8jy56Xh?usp=sharing)
* LightGBM  
The tree is deepened at each step. The accuracy is increased by growing branches from nodes that were not accurate in the previous stage.
* XGBoost  
At each step, the tree is made deeper. The next tree is built to fill in the areas that were not well classified in the previous trees.  
[Sample Code](https://colab.research.google.com/drive/1NdQntsLuqOBJVFjngdVUWqZn1-MUszuz?usp=sharing)

#### ğŸš§Neural NetworkğŸš§
* basic neural network  
[Sample Code](https://colab.research.google.com/drive/1ouVsJgs3CIpDOj_0mZ_rToG5E9ypd0kV?usp=sharing)
<<Note(The neural network sample code uses a GPU.  
Please use with caution since there is a limit to the amount of time and computation available than usual.)>>
* Convolutional Neural Networkï¼ˆCNNï¼‰  
* Recurrent Neural Networkï¼ˆRNNï¼‰  

### 3. Evaluation metrics
Check the prediction accuracy of the model by data not used for training.  

#### Classification Problem  
For classification problems, create a confusion matrix and then check various indicators.  
|                | Predicted Positive | Predicted Negative |
|----------------|--------------------|--------------------|
| **Actual Positive** | TP (True Positive)                 | FN (False Negative)                 |
| **Actual Negative** | FP (False Positive)                  | TN (True Negative)                 |

* Accuracy  
Percentage of correct predictions out of all predictions  
TP + TN / TP + TN + FP + FN
* Precision  
Percentage of predicted positives that are actually positive  
TP / TP + FP
* Recall
Percentage of correctly predicted positives among those that are actually positive  
TP / TP + FN  
* F1 score
Harmonic mean of Precision and Recall  
2 Ã— Precision Ã— Recall / Precision + Recallvv
* AUC-ROC
Indicates whether the sample was correctly sorted  
[Reference](https://arize.com/blog/what-is-auc/)  
* AUC-PR  
Used for unbalanced data. Gives more weight to the accuracy of predictions for samples with higher rankings.  

#### Regression problems
* Mean squared error (MSE)  
* Mean absolute error (MAE)  
* Mean Absolute Percentage Error (MAPE) 

### 4. Hyperparameter tuning
To obtain high prediction accuracy, the values of hyperparameters during training are changed, and training and evaluation with validation data are repeated.
* Grid Search  
This is a method of model building for all patterns of candidate values of a given hyperparameter.  
```
param_grid = {
    â€œXâ€œ: [â€Aâ€, â€˜Bâ€™], # out -> [1.0, 1.2, 1.4, 1.6, 1.7, 1.8 
    â€œY": [i / 10 for i in range(10, 20, 2)], # out -> [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]
}
```  
In the above case, the model is built with 20 patterns of 2 * 10, and the most accurate parameters are chosen.

* Random Search  
Grid search exhaustively searches for combinations, which takes a huge amount of time if one wants to precisely adjust the model.  
Random Search randomly selects values within a specified hyperparameter range and evaluates model performance based on the combinations.  
Because it selects random combinations, it does not fall into local optimization and may find good combinations in a short time.

* Bayesian optimization  
Bayesian optimization does not rely on randomness and aims to find the optimal solution with a minimum number of trials.
A stochastic model (e.g., Gaussian process) is used to model the unknown objective function, and the next trial point is selected by updating the posterior distribution on this model.  
**Sample code implements Bayesian optimization with optuna. **

## å‚è€ƒæ–‡çŒ®ï¼ˆReferencesï¼‰ 
æœ‰è³€å‹ç´€ã€å¤§æ©‹ä¿Šä»‹ã€ŒRã¨Pythonã§å­¦ã¶ã€€å®Ÿè·µçš„ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼†æ©Ÿæ¢°å­¦ç¿’ã€ï¼ˆæŠ€è¡“è©•è«–ç¤¾ã€2019å¹´ï¼‰  
æ¸…æ°´ç§€å¹¸ã€ŒPythonã§å®Ÿè·µ : ç”Ÿå‘½ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿæ¢°å­¦ç¿’ : ã‚ãªãŸã®PCã§æœ€å…ˆç«¯è«–æ–‡ã®è§£æãƒ¬ã‚·ãƒ”ã‚’ä½“å¾—ã§ãã‚‹!ã€ï¼ˆç¾ŠåœŸç¤¾ã€2013å¹´ï¼‰
